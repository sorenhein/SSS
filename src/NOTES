Buglet: range when 0xf should not be 15 minus ...


Can't use CoverRow rather than CoverRowOld at the moment:
symmFlag is not in the looked-up cover, so have to track separately.
This is because we haven't done symmetric cover's generally yet.
Other than that the two are more similar now.
So it's getting to be time...

CoverRow could potentially get a list<Cover const *> already,
so then there's only one add that takes care of resize and weigh.


Time CoverStore vs rest of prepareNew
In particular perhaps CoverStore::add

CoverRow and CoverRowOld can probably be combined somehow.
Covers can be split into Covers and CoverSolve; but Solve is not
really a class with its own memory.  Maybe coverSolve?



Tricks: Optimize into 5-groups

3. Make ProductMemory
   - Usage: Not just flat count, but vector of oppsLength,
     for each Product
   - Sums of these per oppsLength
   - Actual Product's for a given oppsLength, or (oppsLength, topMax)
   - Record #times in tableau, in solution

   - Potentially if we want to store a length-6 1:1?????, it is
     kind of the same as a lengh-6 1:1? with an offset of 4.
     Printing out can be spoofed.
     So the pointer to product memory must also include an offset.
     Matching an include: The reduced #2 is the unreduced #6 etc.
     Several inputs are just skipped as being irrelevant in that case.
     Probably would have to store each offset and each partition that
     gave rise to that product 



10 / 53208, QJ65 / - missing HHxxxx
Not entirely convinced by the Strategy #0 output, either old or new.
The "Either" part in the old one presumably applies when the length
part is also symmetrized.

9 / 6101
East has (xx) -- express differently

ProductMemory
TableauMemory
Need a way to order Term's and Product's

Term
----
Can perhaps move Term to only index & data and nothing else?
Less memory.

Side
----
- We can set up a symmFlag which means that in includes()
  we check length and tops but with (W, E) and (E, W), fooling
  Term in the process.  Actually in Cover/CoverNew we can just mirror
  the results.  But probably the Term's should know that it's 
  now a symmetric one, and not West or East.

- Does Product::strVerbal and strLine still need sumProfile?
  Does Term?

- Term and Product don't know actual lengths
- ProductMemory
- Cover knows symmFlag and actual lengths (or depends on them)
- CoverMemory (new)
  



Names of cards
--------------
Opponents' cards seem to live in ranks/Opponents.
Maybe we shouldn't use HH as in 4/24, but actualy KQ.
Maybe all we really need is to interrogate Opponents?
It seems to use external ranks in rankInfo[r].

Verbals of CoverElement (call it Term):
Maybe inherit into a Length and a Top that has 1-D verbals.

If all terms are singles, it's a single holding.

What else?


Thoughts on runtime and memory of covers
----------------------------------------
It's actually probably a good idea to have the separation between
CoverNew (with a profile etc.) and CoverSetNew (which just has the
cover elements, independent of length and top distribution).

The CoverSetNew's could come from a cover hotel (CoverMemory,
essentially).  There we could keep track of the times they are used.
They would have to be organized in some order.  Perhaps lexically
first by length, then by highest top in its list, ...

CoverMemory would have a pointer or index, probably a pointer, into
the list.

How do we generate the new CoverMemory?  Perhaps in Covers we do
as we do now, including creating the profile, sorting, pruning.
When we are done, we go through the list, and we make another list
with the same number of entries, but with pointers to the cover
memory.  If it exists, the memory is unchanged.  If not, it is
created.  Actually we just keep the CoverSetNew separate in the first
list, and we flesh out the pointer and forget about the CoverSetNew
we created afterwards.

So CoverMemory might have to be an unordered map.

Could reduce a strategy with more complication than actual depth
to the actual depth, similarly to what we already know how to do.
The #cases should add up the right way.

TableauCache
------------
Output of covers

count | depth   0         1         2           3
5               full real full real


4567
...

Sums horizontal and vertical
Also percentages

Should show the amount of duplication.

getCoverHistogram(map<unsigned, unsigned>& histogram) const
Each cover gets an index
They are numbered after they are sorted
Then traverse the map and output the covers that are used.
Last attempt to homologize coverMemory manually.

Move to an unordered_map with custom hash (e.g first nonzero one) and ==.
Move to index by #tops (0, 1, ...)
Move to 2-bit groups

Add symmetry wherever there is no self overlap, complexity unchanged

Look at timing
Look at differences with Greedy
Delete old manual code

- Introduce back symmetry somehow
  11/57967, "Either opponent has a singleton honor" comes out to the
  same as "West has between 1 and 2 tops", and even with a symmetric
  play, when we sort and compare, the other one may win.  So I guess
  we prefer either symmetrized ones, or one with no overlap with the OR
  (MECE)
- Delete the original code in CoverMemory and many other places

- Where does the actual name of the card come from?  Ranks?


Maybe a diagonal cover (1,2;1) meaning 1-2 tricks and 1 more length,
i.e. one x?

Verbals automatically
=====================
The greedy algorithm actually doesn't miss anything now.
Neither does the exhaustive one.

% perl extract.pl a
Case    Same  Diff      Same  Diff
2-1        3     -      7361     -
2-2        1     -      1018     -
3-1       13     -     21198     -
3-2        3     -      3608     -
3-3        1     -       738     -
4-1       38    13     16794  3704
4-2       43    25      9203  2550
4-3        6     -      2394     -
4-4        3     -       447     -
5-1       29    27      6201  2828
5-2       40    57      4325  2339
5-3        6    16       659   413
5-4        4     -       505     -
5-5        3     -       124     -
6-1       20     8      2784   411
6-2       20    21      1735   620
6-3        6     3       291    21
6-4        3     1       219     3
6-5        3     -        95     -
6-6        3     -        29     -
7-1        5     -       957     -
7-2        2     3       210    97
7-3        1     2        35    20
8-1        3     -       166     -
8-2        1     1        28     7
9-1        1     -         9     -
----------------------------------
Sum      261   177     81133 13013


Further work:
- Covers::prepareNew
  - Time it
  - See if efficient enough (if not cout'ing)
  - Could do something like strategies/optim/Study.cpp
    So groups of 5, or binary 10x10 = 1m byte entries
    Would speed up prune, but slow down prepare
    Later on, could speed up matching, but then we might prefer
    5-distribution groups as result vectors go up to 2 bits each.
  - Does it help not to make 50k entries in Covers, but say to double
    the size every time we run out of space?
  - Maybe later on we know which covers end up getting used,
    so we store those in a file and we know the count ahead of time.
    Could generate the prepare's ourselves.
    Would avoid prune and sort
- Covers::explainGreedy
  - Time it
  - Check efficiency
- Rework greedy results, e.g. 11/159327, QJ7654 / 9 missing AKT8
  - Ideally we'd first recognize the length-based one, == 2,
    lowest complexity perhaps
  - Then the first one only contributes d = 3: len 1 top 1
  - And the last one contributes d = 9 and 10: len 3 top 2
  - So they become more MECE, and in fact the row can be split into 3
- Try higher depths, both greedily and verbally

* Control its general by the input flag
* The depth (#explicit tops) should be a control parameter somewhere
* Optimize space if needed
  - A trick profile could be rolled up into 16-bit chunks (8 distributions
    times 4 trick options), so maybe up to 16 2-byte numbers = 32 bytes.
    Then look up 8-bit binary trick profiles to see whether they are
    below the full trick profile, before subtracting out normally.
    Both speed and space.  But this is an optimization.


Don't loop over last top level (could be only top level).
No entry that is completely empty or completely full.


Verbals of strategy wins
========================
* When of the form (1 top) - hierarchical children, perhaps
  - Take 2 tricks (but one less when West is void), and more when...
  - Count that as 1 less explanation?

* Look at text for all the couple hundred entries directly

* ResExpl::str method, for each strategy
  - Would be nice to parametrize by the actual names of cards

* If the same profile arises in two ways, pick the less complex one?
  - Give each cover a complexity that matters when sorting

* Also output differences between two strategies
  - As text, maybe mainly actual combinations (Hx, HHx, ...)
  - In a table (as Unicode text, HTML, ...)
  
* Store and calculate in the compressed format of Study?  Can we
  even subtract stuff directly in that domain?

* Have to get down at least to the second rank
  - Even more Cover's?

* Also forget covers?


2022-01-15
==========
Optimizations to give up or postpone:

1. Early consolidation
   * consolidate() is only called internally in Slist and in 5 places
     - End of adapt: lessEqualCompleteBasic, if 2+ strategies
     - End of reduceByResults if something changed:
       lessEqualCompleteStudied or lessEqualPrimaryStudied
     - End of expand, if list has become unordered:
       lessEqualCompleteStudied or lessEqualPrimaryStudied
     - multiply by a Strategy, if consolidateFlag, lessEqualMethod
       - Called from Strategies::reactivate, once without and once with c.
         After multiplying simple strategy on.
         lessEqualPrimaryStudied.
       - Looks like only place that consolidates.
     - End of purgeRanges, if something erased:
       lessEqualPrimaryStudied

I think we need a gentler compare for intermediate consolidations.
Something is only dominated if both tricks and winner are dominated.

In Result, things to look at:
*= not sure what to do here.  Flag ambiguity and study it?
+= should perhaps assert that there is no ambiguity.
compareComplete: soft version
compareInDetail: soft version


THOUGHTS ON PARTITIONING

Four criteria to consider:

1. Ordered: North has the highest of the NS cards (or both NS are void).

2. Canonical: North has the >= highest count of each rank among NS down 
   to the first one where it makes a difference.  So always ordered.
   If there are any irrelevant low cards, then these are ordered
   within ranks, but there may be irrelevant ranks.

3. Minimal: Canonically ordered.  Needs itself for at least one strategy.  
   Everything below the corresponding rank is ordered.

Later on we will only solve canonicals for the user, so there is little
point (other than pride) in solving non-canonicals for ranks.

I think we cannot reach a non-canonical from a canonical one.  We might
get a rotation, but that doesn't alter the fact.

We wouldn't solve a non-minimal canonical for the user.  But a minimal
canonical might need a non-minimal canonical, I think.  Or not?

We can solve everything up to <= 9 cards and keep it, even if there are
quite a few strategies.

For a 13-card combination that needs a very large number of strategies,
as long as we can calculate it:  We can look at the number of 
strategies if we cut off ranks ever higher, and stop when the number
of trick-distinct strategies is small enough that we're willing to
solve for it.  Then it's a reference to that strategy.

For exactly 10 cards with a South void, we might hamper 13-carders
by limiting the number of strategies that way.  Or not?

If we only rank-solve canonicals, presumably we have fewer rank
problems and fewer ranked strategies as well.  So testing might be
more benign?

2022-01-07
==========

All old tests I can think of:

Play average 3.44
Play average 7.93
Play average 15.51
Play average 27.03
Play average 43.36
Play average 65.51
Play average 94.54

CombTest errors
---------------

WARNING      229 uses non-minimals
MISMATCH    2980 minimals don't add up

Other checks not implemented yet
--------------------------------
* Ranks: If NS" and N"S, also N'S' and nothing else, etc.

* Unranked strategies should all be among ranked ones, and no
  ranked one should go away in terms of its number of tricks


Ideas against errors
--------------------

I'm beginning to think that the only way to fix the rank stuff is to
do all calculations up to the last step of a trick using only complete
dominances, i.e. tricks do not count more than ranks.

Then in the end we can think of:
- Making small cards irrelevant
- Consolidating also on tricks alone

Indirect influence
..................

It would be nice if all WARN cases lead to the same kind of MISMATCH.
But four of them (up to 11 cards) don't lead to a mismatch at all.
They're all in a way the same.

11/53816, AJ6 / T875
7 strategies, of which 4 use T, 2 use 6 and 1 uses 7.
When we drop the 5 (11/53818, AJ6 / T874), it's the same.

When we drop the 6 (11/53830, AJ5 / T874), we'd expect the same.
But we only get 5 strategies now.

In 11/53816, it is possible to derive a strategy from the start T9J,
leaving A6 / 875.  If it loses to an honor (d=8, H94/H), we can still
finesse with the 87 without the 6.

Against d=11 (KQ94/void) after this start, we have A6 / 875 missing KQ4.
Here we can take 3 tricks using the six.  So the strategy survives,
as it takes the same number of tricks as others.

Later on we find that West can cover, THA leaving J6 / 874 missing
H95.  Here we only take 2 tricks leading to the jack; West hops up
and later scores the 9.

So the strategy "forgets" that it needed the 6 for something.

In 11/53830, if we start T9J leaving A5 / 874, we also get 3 tricks
using the 7 against d=8.  But against d=11 we are left with
A5 / 874 missing KQ6, and now we can only take 2 tricks.  So the
strategy is eliminated completely, as it takes fewer tricks even
though it might have a USP in terms of ranks somewhere.

So it's similar to the premature eliminations below.


Grosvenor gambit
................
9/2076, AK6 / T9
From d = 4 (Hx / Hx), defenders may drop HH on the 9.  Then it looks
like 5/51, AT / K missing QJ, where there is only one solution for
all distributions, 2/AK which translates into the ten.

The defense would never do this from d = 2 (H / Hxx), as it would show
up when we lead the T, so then it would just cost a trick.

From d = 6 (Hxx / H) it would cost a trick if declarer plays for it.
But that possibility is not easily eliminated early on.  When the
defense multiplies 9x and 9H, it will always go for 9x with d = 6.

But even if declarer figured this out, he would still end up in
5/51, this time "knowing" that the central distribution (x/x) is the
only one.  Unfortunately the strategies has 2/AK because cashing the
ace has the result profile 1/A, 2/A, 1/A which lost out to
2/AK, 2/AK, 2/AK due to fewer tricks.

9/2076 refers to 9/2098, AK7 / T6 which only ever uses the king.
The same thing could and does happen here if declarer leads the 6:
The defense may crash the honors.  But declarer can also lead the ten,
sacricifing the card!  Now there is nothing to crash.  Declarer can
get the ten out of the way here and not in 9/2076.

So 9/2098 refers to 9/2164, AK8 / 76 which also uses only the king.

We can in principle keep strategies such as 1A 2A 1A vs 2AK 2AK 2AK
alive within a trick, but it is very unappealing to do this across
tricks.

So maybe we recognize when the parent has a worse profile than a
minimal, and we count (and fix?) these.


Premature reduction
...................

In 9/2645 (AQ/KJ9 missing Txxx), if we start with the 9, we get
#0 the trivial strategy (always needing AKQ), 
#1 9 to A, then overtake Q if the T shows up, wins a rank (AK) for d=3,
loses a rank (QJ) for d=1,
#2 9 to A, then always overtake Q, wins ranks (AK) for d=3,4,
loses ranks (QJ) for d=1,2.

If we start with J (which doesn't make sense), we get the same
trivial strategy #0; we don't get #2 as we can't afford always to
overtake; instead we get #1 which is similar to #1 above but
needing the 9 and not the QJ for d=1.
The #1 from J should lose to the #1 from 9.  But the #1 from 9
has already been eliminated (reduced), so #1 from 9 seems to survive.

This wouldn't show up in a minimals check, as the 9 would seem to
be needed (so 9/2645 would be minimal, needing the 9). The one needing
the jack would presumably be minimal as well.

The only solution seems to be not to reduce before the += step.
How much time and memory does this cost?

Update: Now that we don't lead the jack, it can't happen anymore.


Premature consolidation
.......................

In 7/1595 (KT9/Q) for the start Q J there are two strategies, each 
with pluses and minuses for declarer.  #1 (overtaking) needs 2N' 
in order not to lose on tricks anywhere to #0.  Then d = 7 (Kxx / x)
from Q x and Q A comes into play (1, KQ) and beats d = 7 in both
these strategies, so 2N' is no longer needed.  #0 and #1 survive.

In 7/1599 (KT8/Q), #1 loses on tricks to #0 as the 9 is missing,
even though #1 would win d = 6 on ranks. Again d = 7 is eliminated,
so now #1 would be competitive against #0, but it is too late.

The solution seems to be not to consolidate in the partner += step
already, and to wait for the lead += step (and the LHO *= step).
This probably costs memory and time.  I think I will wait with this
until I've done some other optimizations.  Or actually, only
consolidate away strategy's that are completely dominated, i.e.
every result is <=.

How it shows up: In 7/1595 there are 2 strategies, in the combination
of the two "minimals" only 1 (the same).  There should be 2.

Rare products
.............

In 8/1799 (AT/QJ9 missing Kxx) for the start Q and for d = 7 (Kxx/-),
West can duck and then declarer finesses (#0): 2, 2S as superficially,
only the queen is needed.  Later we multiply with Q K (cover) which
has 3/2NS' (QJT).  West will not cover, so it looks as if 2/2S is 
enough.  But 3/2NS' needs a lot more cards than 2/2S.  Concretely
it is in some sense possible to play Q K A for only 2 tricks by
somehow conceding after winning the jack.

3/2NS' in a sense has a profile of trick-taking capability:
3/2NS', 2/2S', 1/4N.  If we multiply this by 2/2S it should yield
2/2S' and not 2/2S.  The condition is more tricks with 2+ more
constraints in between them.  Then it may be possible to weaken
and still improve on the bound.

A trivial strategy stays trivial.

When adapting, we have a new winner (say 1/C) and a subsequent winner
(say 1/D).  We can make 2/(C*D).  We can also make 1/C or 1/D.
If C is more constrained, then we get 2/C and 1/D (as declarer gets
to choose the less constrained one if he gives up a trick).
1/D is only interesting if it is 2+ more relaxed than C.

In general, we can multiply one profile with another to preserve the
sums of tricks.  Then we can pick the most constrained way to get
a number of tricks (defense chooses).  Then we can prune some
declarer's choices of giving up tricks if it doesn't gain 2+ cards.

Again this takes memory and time.

if (tricks > result.tricks)
{
  if (! winner.empty() &&
      winner.getAbsNumber() + 1 < result.winner.getAbsNumber())
  {
  }
}
else if (tricks < result.tricks)
{
  if (! winner.empty() &&
      result.winner.getAbsNumber() + 1 < winner.getAbsNumber())
  {
  }
}

How it shows up: 3 strategies in 1799, 2 in the only minimal (1799).
In this case the third one shouldn't actually exist.

Overall partitioning
--------------------

* Can characterize solved, ranked combinations as canonical or not,
  and minimal or not (bit vector of bools?)

* CombEntry:cpp, e.g. setMinimal: review

Symmetrization
--------------

* For void starting with 10-13 cards (assuming we solve up to 13
  cards), it's always OK to symmetrize, as we can never get there
  from non-void combinations.  We do this:
  a. If we get above [100] strategies, we start over and only
     look up symmetrized strategies (so 6-8 and 8-6 are done together
     and symmetrized against each other).  Multiply strategy by
     strategy, I think, and don't do a complete cross
  b. In the end, if we are above [16] strategies, we symmetrize
  c. If we know from lookup that something must be symmetric

* A histogram of #strategies shows that we also get large numbers
  when South has a single card, e.g. 11/132887, KJ95 / 7 missing AQT864.
  Maybe it's OK to limit next strategies to symmetric ones, too,
  when South becomes void and there are a lot of next strategies.
  1606 strategies here.

* Symmetrize 11 / 132902, KJ96 / void: From ~ 9619 to 166 strategies.
  Similar to the square root.

* Actually, maybe the way to cut down is to gradually bump up
  the critical rank and eliminate strategies from below, until we
  arrive at a reasonable range?  And check that the eliminated ones
  are covered by trick vectors when we later forget about ranks?

  Like a rolling pruning?  Once cards == 10 are done, go over and
  turn some holdings into references to other because there are
  too many strategies.  So this is a new type, not minimal/
  canonical/oriented, but simplified from one rank to another.
  Just note its own rank, because the referred one also has a rank.


Optimizations
-------------
* If there is a Result per Strategy, equality can be tested first
  in this way.  And then one Result per group (Reduction).
  - One classical lowest result, one that takes history into account
  - In print output, show the result(s)

* Similarly when only < or > is possible.  Take the lowest one and
  its groups.

* Would it be worthwhile to store the minimal numbers in the Strategy
  as well?
  -- Then it would also be in the print output, in the table itself

* Optimize the code for Slist::minimal().  Is it even used?

* Probably Winners can really go away now -- always single-valued?
  MultiResult then too?

* Look at speeding up Slist::equalByMethod.
  Divide completely by size
  assert size(v1) >= size(v2)?

* Also optimize for Strategies == even though it doesn't matter
  - Start at ==, only do upper triangle of matrix

* Is Declarer::greater used?  Is it completely right?

* If space became really tight, the minimals in CombEntry could be
  indices in a central list (careful with multi-threading).  That would
  save the 24-byte overhead in each CombEntry at the cost of two
  4-byte counters and potentially an internal "iterator".  So it
  should save 12-16 bytes per CombEntry, and there are ~ 2m of them,
  so ~ 25 MB.

* Why more than 2x Ranks to Plays and Strategize at the end of output?

* No / limited endl


Ranks stored in tables
----------------------
* Write and read binary files with holdings to run vs. not
* Is it true that a minimal combination does not need non-minimal
  ones to solve?  If not, we would either solve the non-minimal
  ones, or look up the non-minimal one (probably better) and
  map the smaller number of distributions to the current case somehow


Mixed strategies
----------------
In the end we may not have to do so much LP.  EW must find play for
each distribution that work against all NS strategies.  EW get into
trouble when they need to vary their play depending on what NS do.
So we keep track of all plays that work in various situations, and we
take the intersection which is often not empty.

I'm hoping this will also give rise to the mandatory falsecards that
protect not this holding, but some other holding.

Perhaps represent all the NS strategies as some kind of tree with the
branching points that they will actually make use of.

In the end there will be the combinations with mixed potential.  But
I think we can do all the above independent of external constraints.
So we can actually tell in the abstract which combinations have the
potential for mixed strategies, even without knowing the constraints?!


Features
--------
* Distribution::limit, using a struct that Control returns
  - We will fail for now on some HCP values

* Not all features of Control.cpp are implemented yet

* Check sometime that these still work
  - Node optimization (on/off)
  - Strategies *= optimization (on/off)


Semantics, verbal descriptions
------------------------------
* Winning plays/strategies such as "cash ace"
  - Sometimes NS have several ways to reach its optimum; keep them all
* Noting the inequalities on EW plays that are needed to stabilize
  an NS strategy

Bugs
----
Why doesn't it work in Combination to make complete copies of
everything?  It must be some stray pointers, but where?

